{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDA@HSE+YSDA, 2024\n",
    "\n",
    "## Seminar 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade Cython\n",
    "!pip install --upgrade Ripser\n",
    "!pip install --upgrade diagram2vec\n",
    "!pip install --upgrade giotto-tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "vr = Rips()\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "import persim\n",
    "import diagram2vec\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from gtda.diagrams import PersistenceEntropy, PersistenceImage, BettiCurve\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagram_reshape(diagram):\n",
    "    zero_idx = np.where(diagram[:,2]==0)\n",
    "    one_idx = np.where(diagram[:,2]==1)\n",
    "    return diagram[zero_idx], diagram[one_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Persistent diagrams, Wasserstein distance and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "X, y = make_circles(n_samples=200, noise=0.1)\n",
    "X = X[y==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topology studies data invariant to continous transformations, so topological invariants like (persistent) homology will not change under such class of transformations.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Apply rotation and dilation transformations to copy of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.radians(30)\n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array(((c,-s), (s, c)))\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data\n",
    "X_transformed = np.copy(X)\n",
    "X_transformed[:,0] = X[:,0] * 0.75\n",
    "X_transformed = np.dot(X_transformed, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax[0].set_title(\"Data\")\n",
    "ax[0].set_xlim(-1.5, 1.5)\n",
    "ax[0].set_ylim(-1.5, 1.5)\n",
    "ax[0].grid(linestyle=\"dotted\")\n",
    "ax[0].scatter(X[:,0], X[:,1], c=\"b\", alpha=0.5)\n",
    "\n",
    "ax[1].set_title(\"Transformed data\")\n",
    "ax[1].set_xlim(-1.5, 1.5)\n",
    "ax[1].set_ylim(-1.5, 1.5)\n",
    "ax[1].grid(linestyle=\"dotted\")\n",
    "ax[1].scatter(X_transformed[:,0], X_transformed[:,1], c=\"b\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute persistence diagrams of a filtration of Vietoris-Rips complex built on point cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = vr.fit_transform(X)\n",
    "diagram_transformed = vr.fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plt.suptitle(\"Persistent diagram of a filtration\")\n",
    "\n",
    "ax[0].set_title(\"Data\")\n",
    "ax[0].grid(linestyle=\"dotted\")\n",
    "vr.plot(diagram, ax=ax[0])\n",
    "\n",
    "ax[1].set_title(\"Transformed data\")\n",
    "ax[1].grid(linestyle=\"dotted\")\n",
    "vr.plot(diagram_transformed, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define the geometry on the space of persistent diagrams, defining a metric on it. Optimal transport approach is used to compare persistent diagrams which are multisets of intervals of arbitrary cardinality.  \n",
    "\n",
    "The variants of optimal transport distances are _Wasserstein-2 distance_, and its approximations like _sliced Wasserstein distance_ and _Bollteneck distance_, which is Wasserstein-$\\infty$ distance.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Compute Bottleneck `persim.bottleneck` and sliced Wasserstein distances `persim.sliced_wasserstein` between perisistent diagrams of original and transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_transformed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persim.bottleneck(diagram[1], diagram_transformed[1]) # pc w noise=0.1 vs w/ noise 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persim.sliced_wasserstein(diagram[1], diagram_transformed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck distance used a single matching between most discriminative pair of points.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Visualize Bottleneck matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Bottleneck distance matching\n",
    "d, matching = persim.bottleneck(diagram[1], diagram_transformed[1], matching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Bottleneck distance matchign\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.suptitle(\"Bottleneck distance matching\")\n",
    "ax.grid(linestyle=\"dotted\")\n",
    "persim.bottleneck_matching(diagram[1], diagram_transformed[1], matching, labels=['Original $H_1$', 'Transformed $H_1$'], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bottleneck distance stability to small perturbations is theoretically proved.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Plot Bottleneck distance with respect to different level of Gaussian noise applied to original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "\n",
    "n_noise_levels = 11\n",
    "n_repeats = 10\n",
    "\n",
    "# original data\n",
    "X_orig, y = make_circles(n_samples=n_samples, noise=0.0)\n",
    "X_orig = X_orig\n",
    "diagram_orig = vr.fit_transform(X_orig)\n",
    "\n",
    "distances = np.zeros((n_noise_levels, n_repeats))\n",
    "\n",
    "for j in tqdm(range(n_repeats)):\n",
    "    for i, noise_level in enumerate(np.linspace(0, 0.2, n_noise_levels)):\n",
    "\n",
    "        X_noisy, _ = make_circles(n_samples=n_samples, noise=noise_level)\n",
    "        X_noisy = X_noisy\n",
    "\n",
    "        diagram_noise = vr.fit_transform(X_noisy)\n",
    "        \n",
    "        distances[i,j] = persim.bottleneck(diagram_orig[1], diagram_noise[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.set_xlabel(\"Noise level\")\n",
    "ax.set_ylabel(\"Bottleneck distance\")\n",
    "ax.set_xticks(range(n_noise_levels), np.linspace(0, 0.2, n_noise_levels))\n",
    "ax.grid(linestyle=\"dotted\")\n",
    "ax.plot(distances.mean(axis=1), c=\"b\")\n",
    "\n",
    "std = distances.std(axis=1)\n",
    "lower = distances.mean(axis=1) - std\n",
    "upper = distances.mean(axis=1) + std\n",
    "ax.fill_between(range(n_noise_levels), lower, upper, color='b', alpha=.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persistent homology of graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is as follows:\n",
    "\n",
    "1. compute persistent diagrams via Ripser \n",
    "2. compute vectorization of diagrams, so-called persistent images and Betti curves\n",
    "3. apply classifier on vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_graphs = pickle.load(open(\"./data/metric_graphs/X.pkl\", \"rb\"))\n",
    "y_all = pickle.load(open(\"./data/metric_graphs/y_all.pkl\", \"rb\"))\n",
    "y_dnod = pickle.load(open(\"./data/metric_graphs/y_d_nod.pkl\", \"rb\"))\n",
    "\n",
    "y_col = [\"a\"] * len(y)\n",
    "y_col = np.array(y_col)\n",
    "\n",
    "y_col[y==0] = \"blue\"\n",
    "y_col[y==2] = \"green\"\n",
    "y_col[y==1] = \"red\"\n",
    "y_col[y==3] = \"yellow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add h_1 diagrams only\n",
    "maxdim = 1\n",
    "h = 1\n",
    "\n",
    "rips = Rips(maxdim=maxdim)\n",
    "\n",
    "diagrams = []\n",
    "for x in X_graphs:\n",
    "    diagrams.append(rips.fit_transform(x, distance_matrix=True)[h])\n",
    "\n",
    "len(diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(X_graphs)\n",
    "distances = np.zeros((n, n))\n",
    "\n",
    "for i in tqdm(range(0, n)):\n",
    "    for j in range(i+1, n):\n",
    "        distances[i,j] = persim.sliced_wasserstein(diagrams[i], diagrams[j])\n",
    "        \n",
    "distances_symmetrize = distances + distances.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_symmetrize = distances + distances.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, max_iter=3000, eps=1e-9, dissimilarity=\"precomputed\", random_state=1, n_jobs=-1)\n",
    "X_metric = mds.fit(distances_symmetrize).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.scatter(X_metric[:, 0], X_metric[:, 1], c=y_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Persistent diagram is a multiset of intervals of arbitrary length which is can not be handled by methods of machine learning. One possible to solutions besides providing a metric on the space of persistent diagrams is vectorization of diagrams to a vector of fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VietorisRipsPersistence()\n",
    "diagrams = vr.fit_transform(X_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betti curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins_curve = 50\n",
    "\n",
    "betti_curves = BettiCurve(n_bins=n_bins_curve).fit_transform(diagrams)\n",
    "betti_curves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistence image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins_image = 15\n",
    "\n",
    "persistence_images = PersistenceImage(sigma=0.1, n_bins=n_bins_image).fit_transform(diagrams)\n",
    "persistence_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "\n",
    "diagram0, diagram1 = diagram_reshape(diagrams[idx])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(13.5, 4), dpi=200)\n",
    "ax[0].set_title(\"Persistence diagram\")\n",
    "ax[1].set_title(\"Persistence image, dim=1\")\n",
    "ax[2].set_title(\"Betti curve\")\n",
    "\n",
    "# persistence diagram\n",
    "ax[0].scatter(diagram0[:,0], diagram0[:,1], s=10, c=\"r\")\n",
    "ax[0].scatter(diagram1[:,0], diagram1[:,1], s=10, c=\"b\")\n",
    "\n",
    "# persistence image\n",
    "ax[1].imshow(persistence_images[idx][1])\n",
    "\n",
    "# Betti curve\n",
    "ax[2].step(np.linspace(0,1,50), betti_curves[idx,0], color=\"r\", where=\"post\", linewidth=2, label=\"dim=0\")\n",
    "ax[2].step(np.linspace(0,1,50), betti_curves[idx,1], color=\"b\", where=\"post\", linewidth=2, label=\"dim=1\")\n",
    "ax[2].legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "#### Persistence images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_control = persistence_images[y_all==0]\n",
    "X_depression = persistence_images[y_all==1]\n",
    "X = np.concatenate((X_control, X_depression), axis=0)\n",
    "X0 = X[:,0].reshape(-1, n_bins_image**2)\n",
    "X1 = X[:,1].reshape(-1, n_bins_image**2)\n",
    "y = np.concatenate((np.zeros(25), np.ones(25)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l2\", C=5.0, solver=\"liblinear\", random_state=42)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pi_h0 = cross_val_score(clf, X0, y, cv=cv)\n",
    "print(\"Accuracy, PI, H0: {:.4f} ± {:.4f}\".format(np.mean(acc_pi_h0), np.std(acc_pi_h0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pi_h1 = cross_val_score(clf, X1, y, cv=cv)\n",
    "print(\"Accuracy, PI, H1: {:.4f} ± {:.4f}\".format(np.mean(acc_pi_h1), np.std(acc_pi_h1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_control = betti_curves[y_all==0]\n",
    "X_depression = betti_curves[y_all==1]\n",
    "X = np.concatenate((X_control, X_depression), axis=0)\n",
    "X0 = X[:,0].reshape(-1, n_bins_curve)\n",
    "X1 = X[:,1].reshape(-1, n_bins_curve)\n",
    "y = np.concatenate((np.zeros(25), np.ones(25)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_curve_h0 = cross_val_score(clf, X0, y, cv=cv)\n",
    "print(\"Accuracy, Betti curve, H0: {:.4f} ± {:.4f}\".format(np.mean(acc_curve_h0), np.std(acc_curve_h0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_curve_h1 = cross_val_score(clf, X1, y, cv=cv)\n",
    "print(\"Accuracy, Betti curve, H1: {:.4f} ± {:.4f}\".format(np.mean(acc_curve_h1), np.std(acc_curve_h1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistent homology of digital images\n",
    "\n",
    "Persistence Diagrams with Linear Machine Learning Models (Obayashi, Hiraoka), 2017  \n",
    "https://arxiv.org/abs/1706.10082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 300\n",
    "sigma1 = 4\n",
    "sigma2 = 2\n",
    "t = 0.01\n",
    "\n",
    "def generate(N, S, W=300, sigma1=4, sigma2=2, t=0.01, bins=64):\n",
    "\n",
    "    z = np.zeros((N, S, 2))\n",
    "    for n in range(N):\n",
    "        z[n, 0] = np.random.uniform(0, W, size=(2))\n",
    "        for s in range(S-1):\n",
    "            d_1 = np.random.normal(0, sigma1)\n",
    "            d_2 = np.random.normal(0, sigma1)\n",
    "            z[n, s+1, 0] = (z[n, s, 0] + d_1) % W\n",
    "            z[n, s+1, 1] = (z[n, s, 1] + d_2) % W\n",
    "\n",
    "    z_r = z.reshape(N*S, 2)\n",
    "    H, _, _ = np.histogram2d(z_r[:,0], z_r[:,1], bins=bins)\n",
    "    \n",
    "    G = gaussian_filter(H, sigma2)\n",
    "    G[G < t] = 0\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "\n",
    "Generate 100 images accoring to model A and model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.zeros((100,64,64))\n",
    "\n",
    "# class A\n",
    "N = 100\n",
    "S = 30\n",
    "\n",
    "for n in range(50):\n",
    "    images[n] = generate(N, S)\n",
    "    \n",
    "# class B\n",
    "N = 250\n",
    "S = 10\n",
    "\n",
    "for n in range(50):\n",
    "    images[n+50] = generate(N, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.gray()\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.title(\"Class A\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.title(\"Class B\")\n",
    "\n",
    "ax1.imshow(images[int(np.random.uniform(0, 50))])\n",
    "ax2.imshow(images[int(np.random.uniform(51, 100))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = []\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    diags.append(lower_star_img(images[i])[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "persim.plot_diagrams(diags[52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betti_curves = diagram2vec.persistence_curve(diags, m=25)\n",
    "betti_curves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_betti_curves = betti_curves[0]\n",
    "y = np.concatenate((np.zeros(50), np.ones(50)), axis=0)\n",
    "\n",
    "y_col = [\"b\"] * len(y)\n",
    "y_col = np.array(y_col)\n",
    "\n",
    "y_col[y==1] = \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_curve_h0 = cross_val_score(clf, X_betti_curves, y, cv=cv)\n",
    "print(\"Accuracy, Betti curve, H0: {:.4f} ± {:.4f}\".format(np.mean(acc_curve_h0), np.std(acc_curve_h0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute the two-dimensional embeddings using linear and nonlinear techniques learned during the course, given persistent images, Betti curves and pairwise distances between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, SpectralEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = Isomap().fit_transform(X_betti_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.scatter(X_emb[:, 0], X_emb[:, 1], c=y_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep sets\n",
    "\n",
    "#### Problem\n",
    "Persistence diagram is a multiset of vectors $D = \\{(b_i, d_i, h_i)\\}_{i=1}^N$ where $b_i$, $d_i$ are the birth and death times of $i$-th topological feature of dimension $h_i$. The classic approach to introduce persistent diagrams to machine learning is related to distances and kernels defined on the space of diagrams, which takes $O(n^2)$ time to compute. Vectrorization schemes such as persistence [images](https://arxiv.org/abs/1507.06217), [landscapes](https://arxiv.org/abs/1501.00179) or [curves](https://arxiv.org/abs/1904.07768) reduce the time to $O(n)$, yet all of this approaches are more or less fixed.\n",
    "\n",
    "Trainable vectorization allows to learn vector representations of persistence diagrams, optimal w.r.t. the downstream task such as classification or regression. The simplest of such models, [Deep Sets](https://arxiv.org/abs/1703.06114) - $f: (\\mathbb{R}^3)^N \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\{x_1, \\dots, x_N\\}) = \\rho \\left( \\sum_{i=1}^N \\phi(x_i) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "consists of an encoder $\\phi_\\theta: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D$ mapping each diagram point $x_i = (b_i, d_i, h_i)$, with parameters $\\theta$ shared between points, a permutation invariant pooling operation $(\\cdot): (\\mathbb{R}^D)^N \\rightarrow \\mathbb{R}^D$ to obtain a representation of a diagram at whole (particulary for Deep Sets - sum pooling), and a decoder $\\rho: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$ which further transforms the diagram representation. It was [shown](https://arxiv.org/abs/1904.09378) that certain combinations of encoder/pooling/decoder correspond to the fixed representation schemes of persistence diagrams.\n",
    "\n",
    "Deep sets encoder vectorizes each single point independently and does not consider the interdependence between points in the diagram. Thus, the self-attention block from the Transformer model which allows to capture those dependencies is a natural plug-in replacement to the encoder $\\phi$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\Phi_{ATTN}(\\{x_1, \\dots, x_N\\}) = \\left(\\frac{(\\mathbf{W}_q \\mathbf{X})(\\mathbf{W}_k \\mathbf{X})^T}{\\sqrt{D}} \\right)\\mathbf{W}_v\\mathbf{X},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Phi_{ATTN}: (\\mathbb{R}^3)^N \\rightarrow (\\mathbb{R}^D)^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orbit(point_0, r, n=300):\n",
    "    \n",
    "    X = np.zeros([n, 2])\n",
    "    \n",
    "    xcur, ycur = point_0[0], point_0[1]\n",
    "    \n",
    "    for idx in range(n):\n",
    "        xcur = (xcur + r * ycur * (1. - ycur)) % 1\n",
    "        ycur = (ycur + r * xcur * (1. - xcur)) % 1\n",
    "        X[idx, :] = [xcur, ycur]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def generate_orbits(m, rs=[2.5, 3.5, 4.0, 4.1, 4.3], n=300, random_state=None):\n",
    "    \n",
    "    # m orbits, each of n points of dimension 2\n",
    "    orbits = np.zeros((m * len(rs), n, 2))\n",
    "    \n",
    "    # for each r\n",
    "    for j, r in enumerate(rs):\n",
    "\n",
    "        # initial points\n",
    "        points_0 = random_state.uniform(size=(m,2))\n",
    "\n",
    "        for i, point_0 in enumerate(points_0):\n",
    "            orbits[j*m + i] = generate_orbit(points_0[i], rs[j])\n",
    "            \n",
    "    return orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "X_orbit5k = generate_orbits(1000, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(16, 6), dpi=200)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        if i==0:\n",
    "            ax[i,j].set_title(\"Class {}\".format(j+1))\n",
    "        ax[i,j].scatter(X_orbit5k[j*1000+i,:,0], X_orbit5k[j*1000+i,:,1], s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pc = np.concatenate((X_orbit5k[2000:3000], X_orbit5k[4000:5000]))\n",
    "y = np.concatenate((np.zeros(1000), np.ones(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pd(diagrams):\n",
    "    pd = np.zeros((0, 3))\n",
    "\n",
    "    for k, diagram_k in enumerate(diagrams):\n",
    "        diagram_k = diagram_k[~np.isinf(diagram_k).any(axis=1)] # filter infs  \n",
    "        diagram_k = np.concatenate((diagram_k, k * np.ones((diagram_k.shape[0], 1))), axis=1)\n",
    "        pd = np.concatenate((pd, diagram_k))\n",
    "\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "vr = Rips()\n",
    "\n",
    "for x_pc in tqdm(X_pc):\n",
    "    diagram = conv_pd(vr.fit_transform(x_pc))\n",
    "    X.append(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orbit2kDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    tmp_pd, _ = data[0]\n",
    "    \n",
    "    n_batch = len(data)\n",
    "    n_features_pd = tmp_pd.shape[1]\n",
    "    n_points_pd = max(len(pd) for pd, _ in data)\n",
    "    inputs_pd = np.zeros((n_batch, n_points_pd, n_features_pd), dtype=float)\n",
    "    labels = np.zeros(len(data))\n",
    "    \n",
    "    for i, (pd, label) in enumerate(data):\n",
    "        inputs_pd[i][:len(pd)] = pd\n",
    "        labels[i] = label\n",
    "    \n",
    "    return torch.Tensor(inputs_pd), torch.Tensor(labels).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden_enc, n_out_enc, n_hidden_dec=16, n_out_dec=2):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.encoder = Encoder(n_in, n_hidden_enc, n_out_enc)\n",
    "        self.decoder = MLP(n_out_enc, n_hidden_dec, n_out_dec)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z_enc = self.encoder(X)\n",
    "        z = self.decoder(z_enc)\n",
    "        return z\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = Linear(n_in, n_hidden)\n",
    "        self.linear2 = Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mlp = MLP(n_in, n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.mlp(X)\n",
    "        x = X.mean(dim=1) # aggregation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_repeats = 3\n",
    "n_epochs = 100\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "\n",
    "n_train, n_test = 1600, 400\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "dataset = Orbit2kDataset(X, y)\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # data init\n",
    "    dataset_train, dataset_test = random_split(dataset, [n_train, n_test])\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader_test =  DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = DeepSets(n_in=3, n_hidden_enc=16, n_out_enc=8)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Train\", \"Test\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for batch in dataloader_train:\n",
    "            loss_batch = criterion(model(batch[0]), batch[1])\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for batch in dataloader_train:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_train = correct / len(dataloader_train.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "\n",
    "        correct = 0\n",
    "        for batch in dataloader_test:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_test = correct / len(dataloader_test.dataset)\n",
    "        history[repeat_idx,epoch_idx,2] = accuracy_test\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train, accuracy_test))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(n_epochs)\n",
    "loss_ci1 = history.mean(axis=0)[:,0] - history.std(axis=0)[:,0]\n",
    "loss_ci2 = history.mean(axis=0)[:,0] + history.std(axis=0)[:,0]\n",
    "acc_train_ci1 = history.mean(axis=0)[:,1] - history.std(axis=0)[:,1]\n",
    "acc_train_ci2 = history.mean(axis=0)[:,1] + history.std(axis=0)[:,1]\n",
    "acc_test_ci1 = history.mean(axis=0)[:,2] - history.std(axis=0)[:,2]\n",
    "acc_test_ci2 = history.mean(axis=0)[:,2] + history.std(axis=0)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4.5), dpi=200)\n",
    "fig.suptitle(\"Persistence diagrams\", fontsize=14)\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[0].set_ylim(0.39, 2.01)\n",
    "ax[1].set_ylim(0.49, 0.81)\n",
    "ax[0].plot(history.mean(axis=0)[:,0], c=\"g\")\n",
    "ax[0].fill_between(x, loss_ci1, loss_ci2, color=\"g\", alpha=0.1)\n",
    "ax[1].plot(history.mean(axis=0)[:,1], c=\"r\", label=\"Train\")\n",
    "ax[1].plot(history.mean(axis=0)[:,2], c=\"b\", label=\"Test\")\n",
    "ax[1].fill_between(x, acc_train_ci1, acc_train_ci2, color=\"r\", alpha=0.1)\n",
    "ax[1].fill_between(x, acc_test_ci1, acc_test_ci2, color=\"b\", alpha=0.1)\n",
    "ax[1].legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
